nohup: ignoring input
Disable distributed.
WARNING: OMP_NUM_THREADS set to 14, not 1. The computation speed will not be optimized if you use data parallel. It will fail if this PaddlePaddle binary is compiled with OpenBlas since OpenBlas does not support multi-threads.
PLEASE USE OMP_NUM_THREADS WISELY.
/root/miniconda3/lib/python3.8/site-packages/skimage/data/__init__.py:107: DeprecationWarning: 
    Importing file_hash from pooch.utils is DEPRECATED. Please import from the
    top-level namespace (`from pooch import file_hash`) instead, which is fully
    backwards compatible with pooch >= 0.1.
    
  return file_hash(path) == expected_hash
2023-06-08 23:37:58,916 INFO: 
  name: GaussianColorDenoising_NBNet
  model_type: ImageCleanModel
  scale: 1
  num_gpu: 1
  manual_seed: 100
  datasets:[
    train:[
      name: TrainSet
      type: Dataset_GaussianDenoising
      sigma_type: random
      sigma_range: [0, 50]
      in_ch: 3
      dataroot_gt: /root/autodl-tmp/SIDD/data/SIDD_patches/train/groundtruth
      dataroot_lq: none
      geometric_augs: True
      filename_tmpl: {}
      io_backend:[
        type: disk
      ]
      use_shuffle: True
      num_worker_per_gpu: 8
      batch_size_per_gpu: 8
      mini_batch_sizes: [8, 5, 4, 2, 1, 1]
      iters: [184000, 128000, 96000, 72000, 72000, 48000]
      gt_size: 384
      gt_sizes: [128, 160, 192, 256, 320, 384]
      dataset_enlarge_ratio: 1
      prefetch_mode: None
      phase: train
      scale: 1
    ]
    val:[
      name: ValSet
      type: Dataset_GaussianDenoising
      sigma_test: 15
      in_ch: 3
      dataroot_gt: /root/autodl-tmp/SIDD/data/SIDD_patches/val/groundtruth
      dataroot_lq: none
      io_backend:[
        type: disk
      ]
      phase: val
      scale: 1
    ]
  ]
  network_g:[
    type: NBNet
  ]
  path:[
    pretrain_network_g: None
    strict_load_g: True
    resume_state: None
    output: /root/autodl-tmp/Restormer_Paddle-main/exp/
    root: /root/autodl-tmp
    experiments_root: /root/autodl-tmp/Restormer_Paddle-main/exp/experiments/GaussianColorDenoising_NBNet
    models: /root/autodl-tmp/Restormer_Paddle-main/exp/experiments/GaussianColorDenoising_NBNet/models
    training_states: /root/autodl-tmp/Restormer_Paddle-main/exp/experiments/GaussianColorDenoising_NBNet/training_states
    log: /root/autodl-tmp/Restormer_Paddle-main/exp/experiments/GaussianColorDenoising_NBNet
    visualization: /root/autodl-tmp/Restormer_Paddle-main/exp/experiments/GaussianColorDenoising_NBNet/visualization
  ]
  train:[
    total_iter: 700000
    warmup_iter: -1
    use_grad_clip: True
    scheduler:[
      type: CosineAnnealingRestartCyclicLR
      learning_rate: 0.00015
      periods: [184000, 416000]
      restart_weights: [1, 1]
      eta_mins: [0.00015, 1e-06]
    ]
    mixing_augs:[
      mixup: True
      mixup_beta: 1.2
      use_identity: True
    ]
    optim_g:[
      type: AdamW
      weight_decay: 0.0001
      beta1: 0.9
      beta2: 0.999
    ]
    pixel_opt:[
      type: L1Loss
      loss_weight: 1
      reduction: mean
    ]
  ]
  val:[
    window_size: 8
    val_freq: 4000.0
    save_img: False
    rgb2bgr: True
    use_image: False
    max_minibatch: 8
    metrics:[
      psnr:[
        type: calculate_psnr
        crop_border: 0
        test_y_channel: False
      ]
    ]
  ]
  logger:[
    print_freq: 100
    save_checkpoint_freq: 5000.0
    use_tb_logger: False
    wandb:[
      project: None
      resume_id: None
    ]
  ]
  dist_params:[
    backend: nccl
    port: 29500
  ]
  is_train: True
  dist: False
  rank: 0
  world_size: 1

W0608 23:37:58.917832 560441 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.6, Runtime API Version: 11.6
W0608 23:37:58.926388 560441 gpu_resources.cc:91] device: 0, cuDNN Version: 8.4.
2023-06-08 23:38:00,112 INFO: Network: NBNet, with parameters: 10,455,235
2023-06-08 23:38:00,113 INFO: NBNet(
  (ConvBlock1): ConvBlock(
    (block): Sequential(
      (0): Conv2D(3, 32, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Conv2D(32, 32, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (3): LeakyReLU(negative_slope=0.01)
    )
    (conv11): Conv2D(3, 32, kernel_size=[1, 1], data_format=NCHW)
  )
  (pool1): MaxPool2D(kernel_size=2, stride=None, padding=0)
  (skip1): Sequential(
    (0): ConvBlock(
      (block): Sequential(
        (0): Conv2D(32, 32, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Conv2D(32, 32, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (conv11): Conv2D(32, 32, kernel_size=[1, 1], data_format=NCHW)
    )
    (1): ConvBlock(
      (block): Sequential(
        (0): Conv2D(32, 32, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Conv2D(32, 32, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (conv11): Conv2D(32, 32, kernel_size=[1, 1], data_format=NCHW)
    )
    (2): ConvBlock(
      (block): Sequential(
        (0): Conv2D(32, 32, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Conv2D(32, 32, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (conv11): Conv2D(32, 32, kernel_size=[1, 1], data_format=NCHW)
    )
    (3): ConvBlock(
      (block): Sequential(
        (0): Conv2D(32, 32, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Conv2D(32, 32, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (conv11): Conv2D(32, 32, kernel_size=[1, 1], data_format=NCHW)
    )
  )
  (ssa1): SSA(
    (conv1): Conv2D(64, 16, kernel_size=[3, 3], padding=1, data_format=NCHW)
    (relu1): LeakyReLU(negative_slope=0.01)
    (conv2): Conv2D(16, 16, kernel_size=[3, 3], padding=1, data_format=NCHW)
    (relu2): LeakyReLU(negative_slope=0.01)
    (conv11): Conv2D(64, 16, kernel_size=[1, 1], data_format=NCHW)
  )
  (ConvBlock2): ConvBlock(
    (block): Sequential(
      (0): Conv2D(32, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (3): LeakyReLU(negative_slope=0.01)
    )
    (conv11): Conv2D(32, 64, kernel_size=[1, 1], data_format=NCHW)
  )
  (pool2): MaxPool2D(kernel_size=2, stride=None, padding=0)
  (skip2): Sequential(
    (0): ConvBlock(
      (block): Sequential(
        (0): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (conv11): Conv2D(64, 64, kernel_size=[1, 1], data_format=NCHW)
    )
    (1): ConvBlock(
      (block): Sequential(
        (0): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (conv11): Conv2D(64, 64, kernel_size=[1, 1], data_format=NCHW)
    )
    (2): ConvBlock(
      (block): Sequential(
        (0): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (conv11): Conv2D(64, 64, kernel_size=[1, 1], data_format=NCHW)
    )
  )
  (ssa2): SSA(
    (conv1): Conv2D(128, 16, kernel_size=[3, 3], padding=1, data_format=NCHW)
    (relu1): LeakyReLU(negative_slope=0.01)
    (conv2): Conv2D(16, 16, kernel_size=[3, 3], padding=1, data_format=NCHW)
    (relu2): LeakyReLU(negative_slope=0.01)
    (conv11): Conv2D(128, 16, kernel_size=[1, 1], data_format=NCHW)
  )
  (ConvBlock3): ConvBlock(
    (block): Sequential(
      (0): Conv2D(64, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (3): LeakyReLU(negative_slope=0.01)
    )
    (conv11): Conv2D(64, 128, kernel_size=[1, 1], data_format=NCHW)
  )
  (pool3): MaxPool2D(kernel_size=2, stride=None, padding=0)
  (skip3): Sequential(
    (0): ConvBlock(
      (block): Sequential(
        (0): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (conv11): Conv2D(128, 128, kernel_size=[1, 1], data_format=NCHW)
    )
    (1): ConvBlock(
      (block): Sequential(
        (0): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (conv11): Conv2D(128, 128, kernel_size=[1, 1], data_format=NCHW)
    )
  )
  (ssa3): SSA(
    (conv1): Conv2D(256, 16, kernel_size=[3, 3], padding=1, data_format=NCHW)
    (relu1): LeakyReLU(negative_slope=0.01)
    (conv2): Conv2D(16, 16, kernel_size=[3, 3], padding=1, data_format=NCHW)
    (relu2): LeakyReLU(negative_slope=0.01)
    (conv11): Conv2D(256, 16, kernel_size=[1, 1], data_format=NCHW)
  )
  (ConvBlock4): ConvBlock(
    (block): Sequential(
      (0): Conv2D(128, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (3): LeakyReLU(negative_slope=0.01)
    )
    (conv11): Conv2D(128, 256, kernel_size=[1, 1], data_format=NCHW)
  )
  (pool4): MaxPool2D(kernel_size=2, stride=None, padding=0)
  (skip4): Sequential(
    (0): ConvBlock(
      (block): Sequential(
        (0): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (1): LeakyReLU(negative_slope=0.01)
        (2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (conv11): Conv2D(256, 256, kernel_size=[1, 1], data_format=NCHW)
    )
  )
  (ssa4): SSA(
    (conv1): Conv2D(512, 16, kernel_size=[3, 3], padding=1, data_format=NCHW)
    (relu1): LeakyReLU(negative_slope=0.01)
    (conv2): Conv2D(16, 16, kernel_size=[3, 3], padding=1, data_format=NCHW)
    (relu2): LeakyReLU(negative_slope=0.01)
    (conv11): Conv2D(512, 16, kernel_size=[1, 1], data_format=NCHW)
  )
  (ConvBlock5): ConvBlock(
    (block): Sequential(
      (0): Conv2D(256, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (3): LeakyReLU(negative_slope=0.01)
    )
    (conv11): Conv2D(256, 512, kernel_size=[1, 1], data_format=NCHW)
  )
  (upv6): Conv2DTranspose(512, 256, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
  (ConvBlock6): ConvBlock(
    (block): Sequential(
      (0): Conv2D(512, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (3): LeakyReLU(negative_slope=0.01)
    )
    (conv11): Conv2D(512, 256, kernel_size=[1, 1], data_format=NCHW)
  )
  (upv7): Conv2DTranspose(256, 128, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
  (ConvBlock7): ConvBlock(
    (block): Sequential(
      (0): Conv2D(256, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (3): LeakyReLU(negative_slope=0.01)
    )
    (conv11): Conv2D(256, 128, kernel_size=[1, 1], data_format=NCHW)
  )
  (upv8): Conv2DTranspose(128, 64, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
  (ConvBlock8): ConvBlock(
    (block): Sequential(
      (0): Conv2D(128, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (3): LeakyReLU(negative_slope=0.01)
    )
    (conv11): Conv2D(128, 64, kernel_size=[1, 1], data_format=NCHW)
  )
  (upv9): Conv2DTranspose(64, 32, kernel_size=[2, 2], stride=[2, 2], data_format=NCHW)
  (ConvBlock9): ConvBlock(
    (block): Sequential(
      (0): Conv2D(64, 32, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (1): LeakyReLU(negative_slope=0.01)
      (2): Conv2D(32, 32, kernel_size=[3, 3], padding=1, data_format=NCHW)
      (3): LeakyReLU(negative_slope=0.01)
    )
    (conv11): Conv2D(64, 32, kernel_size=[1, 1], data_format=NCHW)
  )
  (conv10): Conv2D(32, 3, kernel_size=[3, 3], padding=1, data_format=NCHW)
)
2023-06-08 23:38:00,214 INFO: Training statistics:
	Number of train images: 96000
	Dataset enlarge ratio: 1
	Batch size per gpu: 8
	World size (gpu number): 1
	Require iter number per epoch: 12000
	Total epochs: 59; iters: 700000.
2023-06-08 23:38:00,215 INFO: Number of val images/folders in ValSet: 1280
2023-06-08 23:38:00,215 INFO: Start training from epoch: 0, iter: 0
2023-06-08 23:38:01,014 INFO: 
 Updating Patch_Size to 128 and Batch_Size to 8 

2023-06-08 23:38:20,713 INFO: epoch:0, iter:100, lr: 0.000150 loss: 0.292996  eta: 1 day, 15:27:23, time (data): 0.173
2023-06-08 23:38:36,778 INFO: epoch:0, iter:200, lr: 0.000150 loss: 0.154126  eta: 1 day, 11:21:34, time (data): 0.159
2023-06-08 23:38:52,846 INFO: epoch:0, iter:300, lr: 0.000150 loss: 0.120828  eta: 1 day, 9:59:04, time (data): 0.160
2023-06-08 23:39:08,944 INFO: epoch:0, iter:400, lr: 0.000150 loss: 0.110565  eta: 1 day, 9:18:26, time (data): 0.160
2023-06-08 23:39:25,052 INFO: epoch:0, iter:500, lr: 0.000150 loss: 0.103964  eta: 1 day, 8:54:09, time (data): 0.161
2023-06-08 23:39:41,145 INFO: epoch:0, iter:600, lr: 0.000150 loss: 0.085569  eta: 1 day, 8:37:34, time (data): 0.162
2023-06-08 23:39:57,267 INFO: epoch:0, iter:700, lr: 0.000150 loss: 0.085687  eta: 1 day, 8:26:07, time (data): 0.161
2023-06-08 23:40:13,499 INFO: epoch:0, iter:800, lr: 0.000150 loss: 0.085933  eta: 1 day, 8:19:04, time (data): 0.159
2023-06-08 23:40:29,576 INFO: epoch:0, iter:900, lr: 0.000150 loss: 0.056078  eta: 1 day, 8:11:30, time (data): 0.160
2023-06-08 23:40:45,662 INFO: epoch:0, iter:1000, lr: 0.000150 loss: 0.054429  eta: 1 day, 8:05:31, time (data): 0.160
2023-06-08 23:41:01,691 INFO: epoch:0, iter:1100, lr: 0.000150 loss: 0.056632  eta: 1 day, 7:59:57, time (data): 0.160
2023-06-08 23:41:17,808 INFO: epoch:0, iter:1200, lr: 0.000150 loss: 0.046216  eta: 1 day, 7:56:08, time (data): 0.162
2023-06-08 23:41:33,893 INFO: epoch:0, iter:1300, lr: 0.000150 loss: 0.053862  eta: 1 day, 7:52:34, time (data): 0.160
2023-06-08 23:41:49,953 INFO: epoch:0, iter:1400, lr: 0.000150 loss: 0.053803  eta: 1 day, 7:49:16, time (data): 0.159
2023-06-08 23:42:05,997 INFO: epoch:0, iter:1500, lr: 0.000150 loss: 0.054469  eta: 1 day, 7:46:15, time (data): 0.160
2023-06-08 23:42:22,071 INFO: epoch:0, iter:1600, lr: 0.000150 loss: 0.056040  eta: 1 day, 7:43:48, time (data): 0.161
2023-06-08 23:42:38,126 INFO: epoch:0, iter:1700, lr: 0.000150 loss: 0.043450  eta: 1 day, 7:41:28, time (data): 0.161
2023-06-08 23:42:54,215 INFO: epoch:0, iter:1800, lr: 0.000150 loss: 0.036010  eta: 1 day, 7:39:35, time (data): 0.161
2023-06-08 23:43:10,311 INFO: epoch:0, iter:1900, lr: 0.000150 loss: 0.037821  eta: 1 day, 7:37:55, time (data): 0.160
2023-06-08 23:43:26,416 INFO: epoch:0, iter:2000, lr: 0.000150 loss: 0.041236  eta: 1 day, 7:36:27, time (data): 0.161
2023-06-08 23:43:42,495 INFO: epoch:0, iter:2100, lr: 0.000150 loss: 0.041481  eta: 1 day, 7:34:56, time (data): 0.160
2023-06-08 23:43:58,605 INFO: epoch:0, iter:2200, lr: 0.000150 loss: 0.030102  eta: 1 day, 7:33:42, time (data): 0.160
2023-06-08 23:44:14,697 INFO: epoch:0, iter:2300, lr: 0.000150 loss: 0.039293  eta: 1 day, 7:32:28, time (data): 0.161
2023-06-08 23:44:30,768 INFO: epoch:0, iter:2400, lr: 0.000150 loss: 0.037445  eta: 1 day, 7:31:13, time (data): 0.161
2023-06-08 23:44:46,833 INFO: epoch:0, iter:2500, lr: 0.000150 loss: 0.032724  eta: 1 day, 7:30:00, time (data): 0.160
2023-06-08 23:45:02,895 INFO: epoch:0, iter:2600, lr: 0.000150 loss: 0.032317  eta: 1 day, 7:28:51, time (data): 0.160
2023-06-08 23:45:18,960 INFO: epoch:0, iter:2700, lr: 0.000150 loss: 0.030653  eta: 1 day, 7:27:47, time (data): 0.160
2023-06-08 23:45:35,022 INFO: epoch:0, iter:2800, lr: 0.000150 loss: 0.026853  eta: 1 day, 7:26:46, time (data): 0.160
2023-06-08 23:45:51,099 INFO: epoch:0, iter:2900, lr: 0.000150 loss: 0.028525  eta: 1 day, 7:25:51, time (data): 0.160
2023-06-08 23:46:07,192 INFO: epoch:0, iter:3000, lr: 0.000150 loss: 0.028482  eta: 1 day, 7:25:02, time (data): 0.160
2023-06-08 23:46:23,283 INFO: epoch:0, iter:3100, lr: 0.000150 loss: 0.028367  eta: 1 day, 7:24:16, time (data): 0.160
2023-06-08 23:46:39,360 INFO: epoch:0, iter:3200, lr: 0.000150 loss: 0.022253  eta: 1 day, 7:23:28, time (data): 0.160
2023-06-08 23:46:55,432 INFO: epoch:0, iter:3300, lr: 0.000150 loss: 0.023165  eta: 1 day, 7:22:41, time (data): 0.160
2023-06-08 23:47:11,482 INFO: epoch:0, iter:3400, lr: 0.000150 loss: 0.031321  eta: 1 day, 7:21:51, time (data): 0.161
2023-06-08 23:47:27,568 INFO: epoch:0, iter:3500, lr: 0.000150 loss: 0.027373  eta: 1 day, 7:21:10, time (data): 0.160
2023-06-08 23:47:43,644 INFO: epoch:0, iter:3600, lr: 0.000150 loss: 0.016850  eta: 1 day, 7:20:29, time (data): 0.162
2023-06-08 23:47:59,686 INFO: epoch:0, iter:3700, lr: 0.000150 loss: 0.023380  eta: 1 day, 7:19:43, time (data): 0.164
2023-06-08 23:48:15,765 INFO: epoch:0, iter:3800, lr: 0.000150 loss: 0.021894  eta: 1 day, 7:19:05, time (data): 0.160
2023-06-08 23:48:31,828 INFO: epoch:0, iter:3900, lr: 0.000150 loss: 0.021578  eta: 1 day, 7:18:25, time (data): 0.161
2023-06-08 23:48:47,911 INFO: epoch:0, iter:4000, lr: 0.000150 loss: 0.021386  eta: 1 day, 7:17:50, time (data): 0.165
2023-06-08 23:49:04,051 INFO: epoch:0, iter:4100, lr: 0.000150 loss: 0.019661  eta: 1 day, 7:17:26, time (data): 0.160
2023-06-08 23:49:20,143 INFO: epoch:0, iter:4200, lr: 0.000150 loss: 0.017258  eta: 1 day, 7:16:54, time (data): 0.162
2023-06-08 23:49:36,206 INFO: epoch:0, iter:4300, lr: 0.000150 loss: 0.016338  eta: 1 day, 7:16:18, time (data): 0.162
2023-06-08 23:49:52,291 INFO: epoch:0, iter:4400, lr: 0.000150 loss: 0.024956  eta: 1 day, 7:15:46, time (data): 0.162
2023-06-08 23:50:08,661 INFO: epoch:0, iter:4500, lr: 0.000150 loss: 0.013672  eta: 1 day, 7:16:00, time (data): 0.169
2023-06-08 23:50:24,796 INFO: epoch:0, iter:4600, lr: 0.000150 loss: 0.018018  eta: 1 day, 7:15:36, time (data): 0.161
2023-06-08 23:50:40,834 INFO: epoch:0, iter:4700, lr: 0.000150 loss: 0.023202  eta: 1 day, 7:14:58, time (data): 0.168
2023-06-08 23:50:56,912 INFO: epoch:0, iter:4800, lr: 0.000150 loss: 0.012589  eta: 1 day, 7:14:27, time (data): 0.161
2023-06-08 23:51:12,940 INFO: epoch:0, iter:4900, lr: 0.000150 loss: 0.017229  eta: 1 day, 7:13:50, time (data): 0.160
2023-06-08 23:51:28,949 INFO: epoch:0, iter:5000, lr: 0.000150 loss: 0.014144  eta: 1 day, 7:13:11, time (data): 0.161
2023-06-08 23:51:44,954 INFO: epoch:0, iter:5100, lr: 0.000150 loss: 0.018050  eta: 1 day, 7:12:32, time (data): 0.159
2023-06-08 23:52:00,993 INFO: epoch:0, iter:5200, lr: 0.000150 loss: 0.018436  eta: 1 day, 7:11:59, time (data): 0.160
2023-06-08 23:52:17,033 INFO: epoch:0, iter:5300, lr: 0.000150 loss: 0.014758  eta: 1 day, 7:11:26, time (data): 0.162
2023-06-08 23:52:33,088 INFO: epoch:0, iter:5400, lr: 0.000150 loss: 0.025402  eta: 1 day, 7:10:56, time (data): 0.160
2023-06-08 23:52:49,119 INFO: epoch:0, iter:5500, lr: 0.000150 loss: 0.013099  eta: 1 day, 7:10:23, time (data): 0.161
2023-06-08 23:53:05,162 INFO: epoch:0, iter:5600, lr: 0.000150 loss: 0.015504  eta: 1 day, 7:09:53, time (data): 0.162
2023-06-08 23:53:21,211 INFO: epoch:0, iter:5700, lr: 0.000150 loss: 0.015519  eta: 1 day, 7:09:23, time (data): 0.160
2023-06-08 23:53:37,239 INFO: epoch:0, iter:5800, lr: 0.000150 loss: 0.016834  eta: 1 day, 7:08:52, time (data): 0.160
2023-06-08 23:53:53,259 INFO: epoch:0, iter:5900, lr: 0.000150 loss: 0.016385  eta: 1 day, 7:08:20, time (data): 0.161
2023-06-08 23:54:09,282 INFO: epoch:0, iter:6000, lr: 0.000150 loss: 0.016599  eta: 1 day, 7:07:49, time (data): 0.161
2023-06-08 23:54:25,339 INFO: epoch:0, iter:6100, lr: 0.000150 loss: 0.019608  eta: 1 day, 7:07:23, time (data): 0.163
2023-06-08 23:54:41,391 INFO: epoch:0, iter:6200, lr: 0.000150 loss: 0.016280  eta: 1 day, 7:06:56, time (data): 0.161
2023-06-08 23:54:57,464 INFO: epoch:0, iter:6300, lr: 0.000150 loss: 0.013067  eta: 1 day, 7:06:32, time (data): 0.162
2023-06-08 23:55:13,510 INFO: epoch:0, iter:6400, lr: 0.000150 loss: 0.010115  eta: 1 day, 7:06:05, time (data): 0.162
2023-06-08 23:55:29,568 INFO: epoch:0, iter:6500, lr: 0.000150 loss: 0.015378  eta: 1 day, 7:05:40, time (data): 0.161
2023-06-08 23:55:45,609 INFO: epoch:0, iter:6600, lr: 0.000150 loss: 0.018097  eta: 1 day, 7:05:13, time (data): 0.160
2023-06-08 23:56:01,636 INFO: epoch:0, iter:6700, lr: 0.000150 loss: 0.010366  eta: 1 day, 7:04:45, time (data): 0.162
2023-06-08 23:56:17,673 INFO: epoch:0, iter:6800, lr: 0.000150 loss: 0.012109  eta: 1 day, 7:04:19, time (data): 0.161
2023-06-08 23:56:33,709 INFO: epoch:0, iter:6900, lr: 0.000150 loss: 0.020827  eta: 1 day, 7:03:53, time (data): 0.160
2023-06-08 23:56:49,720 INFO: epoch:0, iter:7000, lr: 0.000150 loss: 0.011913  eta: 1 day, 7:03:24, time (data): 0.161
2023-06-08 23:57:05,737 INFO: epoch:0, iter:7100, lr: 0.000150 loss: 0.016330  eta: 1 day, 7:02:57, time (data): 0.160
2023-06-08 23:57:21,781 INFO: epoch:0, iter:7200, lr: 0.000150 loss: 0.018343  eta: 1 day, 7:02:32, time (data): 0.162
2023-06-08 23:57:37,798 INFO: epoch:0, iter:7300, lr: 0.000150 loss: 0.013294  eta: 1 day, 7:02:05, time (data): 0.160
2023-06-08 23:57:53,831 INFO: epoch:0, iter:7400, lr: 0.000150 loss: 0.016305  eta: 1 day, 7:01:40, time (data): 0.161
2023-06-08 23:58:10,183 INFO: epoch:0, iter:7500, lr: 0.000150 loss: 0.015052  eta: 1 day, 7:01:45, time (data): 0.163
2023-06-08 23:58:26,243 INFO: epoch:0, iter:7600, lr: 0.000150 loss: 0.024355  eta: 1 day, 7:01:22, time (data): 0.161
2023-06-08 23:58:42,347 INFO: epoch:0, iter:7700, lr: 0.000150 loss: 0.011917  eta: 1 day, 7:01:04, time (data): 0.163
2023-06-08 23:58:58,447 INFO: epoch:0, iter:7800, lr: 0.000150 loss: 0.018134  eta: 1 day, 7:00:45, time (data): 0.163
2023-06-08 23:59:14,551 INFO: epoch:0, iter:7900, lr: 0.000150 loss: 0.013317  eta: 1 day, 7:00:27, time (data): 0.161
2023-06-08 23:59:30,641 INFO: epoch:0, iter:8000, lr: 0.000150 loss: 0.012111  eta: 1 day, 7:00:07, time (data): 0.162
2023-06-08 23:59:46,747 INFO: epoch:0, iter:8100, lr: 0.000150 loss: 0.013564  eta: 1 day, 6:59:49, time (data): 0.161
2023-06-09 00:00:02,830 INFO: epoch:0, iter:8200, lr: 0.000150 loss: 0.014184  eta: 1 day, 6:59:29, time (data): 0.162
2023-06-09 00:00:18,931 INFO: epoch:0, iter:8300, lr: 0.000150 loss: 0.010146  eta: 1 day, 6:59:11, time (data): 0.161
2023-06-09 00:00:35,041 INFO: epoch:0, iter:8400, lr: 0.000150 loss: 0.014309  eta: 1 day, 6:58:53, time (data): 0.161
2023-06-09 00:00:51,137 INFO: epoch:0, iter:8500, lr: 0.000150 loss: 0.008920  eta: 1 day, 6:58:35, time (data): 0.162
2023-06-09 00:01:07,268 INFO: epoch:0, iter:8600, lr: 0.000150 loss: 0.009201  eta: 1 day, 6:58:19, time (data): 0.163
2023-06-09 00:01:23,383 INFO: epoch:0, iter:8700, lr: 0.000150 loss: 0.012304  eta: 1 day, 6:58:02, time (data): 0.163
2023-06-09 00:01:39,472 INFO: epoch:0, iter:8800, lr: 0.000150 loss: 0.014725  eta: 1 day, 6:57:43, time (data): 0.161
2023-06-09 00:01:55,577 INFO: epoch:0, iter:8900, lr: 0.000150 loss: 0.012118  eta: 1 day, 6:57:25, time (data): 0.161
2023-06-09 00:02:11,697 INFO: epoch:0, iter:9000, lr: 0.000150 loss: 0.025389  eta: 1 day, 6:57:08, time (data): 0.161
2023-06-09 00:02:27,795 INFO: epoch:0, iter:9100, lr: 0.000150 loss: 0.014557  eta: 1 day, 6:56:50, time (data): 0.161
2023-06-09 00:02:43,902 INFO: epoch:0, iter:9200, lr: 0.000150 loss: 0.011785  eta: 1 day, 6:56:33, time (data): 0.161
2023-06-09 00:03:00,080 INFO: epoch:0, iter:9300, lr: 0.000150 loss: 0.008462  eta: 1 day, 6:56:21, time (data): 0.178
2023-06-09 00:03:16,162 INFO: epoch:0, iter:9400, lr: 0.000150 loss: 0.011311  eta: 1 day, 6:56:01, time (data): 0.161
2023-06-09 00:03:32,227 INFO: epoch:0, iter:9500, lr: 0.000150 loss: 0.014635  eta: 1 day, 6:55:41, time (data): 0.162
2023-06-09 00:03:48,298 INFO: epoch:0, iter:9600, lr: 0.000150 loss: 0.017026  eta: 1 day, 6:55:21, time (data): 0.159
2023-06-09 00:04:04,341 INFO: epoch:0, iter:9700, lr: 0.000150 loss: 0.013527  eta: 1 day, 6:54:59, time (data): 0.160
2023-06-09 00:04:20,400 INFO: epoch:0, iter:9800, lr: 0.000150 loss: 0.012523  eta: 1 day, 6:54:38, time (data): 0.161
2023-06-09 00:04:36,483 INFO: epoch:0, iter:9900, lr: 0.000150 loss: 0.011545  eta: 1 day, 6:54:19, time (data): 0.161
2023-06-09 00:04:52,684 INFO: epoch:0, iter:10000, lr: 0.000150 loss: 0.009481  eta: 1 day, 6:54:09, time (data): 0.160
2023-06-09 00:05:08,823 INFO: epoch:0, iter:10100, lr: 0.000150 loss: 0.013469  eta: 1 day, 6:53:53, time (data): 0.162
2023-06-09 00:05:24,943 INFO: epoch:0, iter:10200, lr: 0.000150 loss: 0.012088  eta: 1 day, 6:53:37, time (data): 0.161
2023-06-09 00:05:41,056 INFO: epoch:0, iter:10300, lr: 0.000150 loss: 0.013288  eta: 1 day, 6:53:20, time (data): 0.162
2023-06-09 00:05:57,202 INFO: epoch:0, iter:10400, lr: 0.000150 loss: 0.055528  eta: 1 day, 6:53:06, time (data): 0.163
2023-06-09 00:06:13,353 INFO: epoch:0, iter:10500, lr: 0.000150 loss: 0.011308  eta: 1 day, 6:52:51, time (data): 0.162
2023-06-09 00:06:29,505 INFO: epoch:0, iter:10600, lr: 0.000150 loss: 0.010286  eta: 1 day, 6:52:37, time (data): 0.163
2023-06-09 00:06:45,683 INFO: epoch:0, iter:10700, lr: 0.000150 loss: 0.012665  eta: 1 day, 6:52:25, time (data): 0.162
2023-06-09 00:07:01,903 INFO: epoch:0, iter:10800, lr: 0.000150 loss: 0.008054  eta: 1 day, 6:52:15, time (data): 0.161
2023-06-09 00:07:17,988 INFO: epoch:0, iter:10900, lr: 0.000150 loss: 0.010664  eta: 1 day, 6:51:56, time (data): 0.161
2023-06-09 00:07:34,075 INFO: epoch:0, iter:11000, lr: 0.000150 loss: 0.008151  eta: 1 day, 6:51:37, time (data): 0.161
2023-06-09 00:07:50,162 INFO: epoch:0, iter:11100, lr: 0.000150 loss: 0.013740  eta: 1 day, 6:51:19, time (data): 0.162
2023-06-09 00:08:06,257 INFO: epoch:0, iter:11200, lr: 0.000150 loss: 0.010828  eta: 1 day, 6:51:01, time (data): 0.162
2023-06-09 00:08:22,357 INFO: epoch:0, iter:11300, lr: 0.000150 loss: 0.010189  eta: 1 day, 6:50:43, time (data): 0.162
2023-06-09 00:08:38,467 INFO: epoch:0, iter:11400, lr: 0.000150 loss: 0.011135  eta: 1 day, 6:50:26, time (data): 0.160
2023-06-09 00:08:54,570 INFO: epoch:0, iter:11500, lr: 0.000150 loss: 0.010398  eta: 1 day, 6:50:09, time (data): 0.160
2023-06-09 00:09:10,666 INFO: epoch:0, iter:11600, lr: 0.000150 loss: 0.009857  eta: 1 day, 6:49:51, time (data): 0.161
2023-06-09 00:09:26,760 INFO: epoch:0, iter:11700, lr: 0.000150 loss: 0.010660  eta: 1 day, 6:49:33, time (data): 0.161
2023-06-09 00:09:42,839 INFO: epoch:0, iter:11800, lr: 0.000150 loss: 0.009688  eta: 1 day, 6:49:15, time (data): 0.162
2023-06-09 00:09:58,983 INFO: epoch:0, iter:11900, lr: 0.000150 loss: 0.008450  eta: 1 day, 6:49:00, time (data): 0.161
2023-06-09 00:10:15,027 INFO: epoch:0, iter:12000, lr: 0.000150 loss: 0.007394  eta: 1 day, 6:48:39, time (data): 0.159
2023-06-09 00:10:15,095 INFO: Saving models and training states on epoch 0.
2023-06-09 00:12:24,387 INFO: Validation ValSet,		 # psnr: 38.8320
2023-06-09 00:12:24,387 INFO: Saving best models and training states on epoch 0.
Traceback (most recent call last):
  File "train.py", line 257, in <module>
    main()
  File "train.py", line 210, in main
    model.optimize_parameters(current_iter)
  File "/root/autodl-tmp/Restormer_Paddle-main/models/image_restoration_model.py", line 157, in optimize_parameters
    l_pix.backward()
  File "/root/miniconda3/lib/python3.8/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/wrapped_decorator.py", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/framework.py", line 434, in __impl__
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py", line 291, in backward
    core.dygraph_run_backward([self], [grad_tensor],
OSError: (External) ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000000GB memory on GPU 0, 17.921631GB memory has been allocated and available memory is only 5.766174GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
If the above ways do not solve the out of memory problem, you can try to use CUDA managed memory. The command is `export FLAGS_use_cuda_managed_memory=false`.
 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:87)
 (at /paddle/paddle/fluid/imperative/basic_engine.cc:586)

